{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HAYET-MEDINI/YoloX_Real_Time_Object_Detection/blob/main/YoloX_Real_Time_Object_Detection__pfe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrwkLh6u_ORn"
      },
      "source": [
        "#Basic Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "EjFuRWrPABk-",
        "outputId": "30b9ad43-9aaa-42c2-84e0-a242f26b413a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#Mounting Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "EA9frO-5YpPf",
        "outputId": "fb40b56f-453e-4f86-d9c3-dd2a32d8dc16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ZKmehkZDAJI2",
        "outputId": "7dfbb34d-7d70-490a-de0a-fa7d9c651aef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'YOLOX' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "# Cloning the Yolo X Repo\n",
        "\n",
        "!git clone https://github.com/augmentedstartups/YOLOX.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zeCmRjy_cPr"
      },
      "source": [
        "#Installing dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSzc911yAMYq",
        "outputId": "d084db95-3889-4fc6-eb11-b3e1ebaa4734"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/YOLOX\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.0.1)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement onnxruntime==1.8.0 (from versions: 1.15.0, 1.15.1, 1.16.0, 1.16.1, 1.16.2, 1.16.3, 1.17.0, 1.17.1, 1.17.3, 1.18.0, 1.18.1, 1.19.0, 1.19.2, 1.20.0, 1.20.1, 1.21.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for onnxruntime==1.8.0\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Installing requirementes.txt\n",
        "%cd YOLOX\n",
        "!pip3 install -U pip\n",
        "!pip3 install -qr requirements.txt\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "u5SFlAYPAFcp",
        "outputId": "51ba5840-8130-4f1b-e9b6-d57ba09da13e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI\n",
            "  Cloning https://github.com/cocodataset/cocoapi.git to /tmp/pip-req-build-ppgtf30q\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/cocodataset/cocoapi.git /tmp/pip-req-build-ppgtf30q\n",
            "  Resolved https://github.com/cocodataset/cocoapi.git to commit 8c9bcc3cf640524c4c20a9c40e89cb6a2f2fa0e9\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.11/dist-packages (from pycocotools==2.0) (75.1.0)\n",
            "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.11/dist-packages (from pycocotools==2.0) (3.0.12)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pycocotools==2.0) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools==2.0) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "# builing pycocotools\n",
        "#la bibliothèque pycocotools, qui est utilisée pour travailler avec le format de données COCO,\n",
        "#un standard pour les annotations d'images dans les tâches de vision par ordinateur comme la détection d'objets, la segmentation, et plus encore.\n",
        "!pip3 install 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "2TerPljQAlty",
        "outputId": "14080cf4-40c2-4068-885d-938215e9f7cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using pip 25.0.1 from /usr/local/lib/python3.11/dist-packages/pip (python 3.11)\n",
            "Obtaining file:///content/YOLOX\n",
            "  Running command python setup.py egg_info\n",
            "  running egg_info\n",
            "  creating /tmp/pip-pip-egg-info-hiznsbmh/yolox.egg-info\n",
            "  writing /tmp/pip-pip-egg-info-hiznsbmh/yolox.egg-info/PKG-INFO\n",
            "  writing dependency_links to /tmp/pip-pip-egg-info-hiznsbmh/yolox.egg-info/dependency_links.txt\n",
            "  writing top-level names to /tmp/pip-pip-egg-info-hiznsbmh/yolox.egg-info/top_level.txt\n",
            "  writing manifest file '/tmp/pip-pip-egg-info-hiznsbmh/yolox.egg-info/SOURCES.txt'\n",
            "  /usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:529: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "    warnings.warn(msg.format('we could not find ninja.'))\n",
            "  reading manifest file '/tmp/pip-pip-egg-info-hiznsbmh/yolox.egg-info/SOURCES.txt'\n",
            "  adding license file 'LICENSE'\n",
            "  writing manifest file '/tmp/pip-pip-egg-info-hiznsbmh/yolox.egg-info/SOURCES.txt'\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Installing collected packages: yolox\n",
            "  Attempting uninstall: yolox\n",
            "    Found existing installation: yolox 0.1.0\n",
            "    Uninstalling yolox-0.1.0:\n",
            "      Removing file or directory /usr/local/lib/python3.11/dist-packages/yolox.egg-link\n",
            "      Removing pth entries from /usr/local/lib/python3.11/dist-packages/easy-install.pth:\n",
            "      Removing entry: /content/YOLOX\n",
            "      Successfully uninstalled yolox-0.1.0\n",
            "\u001b[33m  DEPRECATION: Legacy editable install of yolox==0.1.0 from file:///content/YOLOX (setup.py develop) is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to add a pyproject.toml or enable --use-pep517, and use setuptools >= 64. If the resulting installation is not behaving as expected, try using --config-settings editable_mode=compat. Please consult the setuptools documentation for more information. Discussion can be found at https://github.com/pypa/pip/issues/11457\u001b[0m\u001b[33m\n",
            "\u001b[0m  Running setup.py develop for yolox\n",
            "    Running command python setup.py develop\n"
          ]
        }
      ],
      "source": [
        "# Installing and Building Yolox\n",
        "!pip3 install -v -e ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTaUaYOQ_ov_"
      },
      "source": [
        "#Getting the Files ready"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mieMe34xBQpv"
      },
      "outputs": [],
      "source": [
        "#creating de weights folder\n",
        "import os\n",
        "if not os.path.exists(\"weights\"):\n",
        "  os.mkdir(\"weights\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "EPFkwGETBhVe"
      },
      "outputs": [],
      "source": [
        "\n",
        "# # Download Weights\n",
        "!wget -nc https://github.com/Megvii-BaseDetection/YOLOX/releases/download/0.1.1rc0/yolox_s.pth -P /content/YOLOX/weights\n",
        "!wget -nc https://github.com/Megvii-BaseDetection/YOLOX/releases/download/0.1.1rc0/yolox_m.pth -P /content/YOLOX/weights\n",
        "!wget -nc https://github.com/Megvii-BaseDetection/YOLOX/releases/download/0.1.1rc0/yolox_l.pth -P /content/YOLOX/weights\n",
        "!wget -nc https://github.com/Megvii-BaseDetection/YOLOX/releases/download/0.1.1rc0/yolox_x.pth -P /content/YOLOX/weights\n",
        "!wget -nc https://github.com/Megvii-BaseDetection/YOLOX/releases/download/0.1.1rc0/yolox_darknet.pth -P /content/YOLOX/weights\n",
        "!wget -nc https://github.com/Megvii-BaseDetection/YOLOX/releases/download/0.1.1rc0/yolox_nano.pth -P /content/YOLOX/weights\n",
        "!wget -nc https://github.com/Megvii-BaseDetection/YOLOX/releases/download/0.1.1rc0/yolox_tiny.pth -P /content/YOLOX/weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "h2AmzEDKBGzP"
      },
      "outputs": [],
      "source": [
        "# Download Sample Video and Image\n",
        "\"\"\"\n",
        "  Sample Video :: https://drive.google.com/u/0/uc?id=1amwyM1kRKy3O53lQbbsUCtDUnPsK51Jt&export=download\n",
        "  Sample Image :: https://drive.google.com/u/0/uc?id=1fKI4aldzz6Y-iAldhncZ6GQlD7Xl3ICV&export=download\n",
        "\"\"\"\n",
        "\n",
        "import gdown\n",
        "\n",
        "gdown.download(\n",
        "    'https://drive.google.com/u/0/uc?id=1fKI4aldzz6Y-iAldhncZ6GQlD7Xl3ICV&export=download',\n",
        "    'Image.jpeg',\n",
        "    quiet = True\n",
        ")\n",
        "\n",
        "gdown.download(\n",
        "    'https://drive.google.com/u/0/uc?id=1PygamUfpcCcCzQsKHu1cwPo4PrlxbNH_&export=download',\n",
        "    'video.mp4',\n",
        "    quiet = False\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEKodorn_1Ly"
      },
      "source": [
        "#Demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdfBhWRwURqC"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4VrQ01eCGNq"
      },
      "source": [
        "```\n",
        "CLI Arguments Details\n",
        "\n",
        "   demo   =  Choose between [image, video, webcam]\n",
        " --name   = [yolox-s, yolox-x,......]\n",
        " --ckpt   = Path to the wieghts file for yolox model\n",
        " --path   = file path\n",
        " --conf   = 0.25\n",
        " --nms    = 0.45\n",
        " --device = [gpu,cpu]\n",
        " --save_result\n",
        "\n",
        " Have used backward slashes to unbreak line \\\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWHULQG7CR_B"
      },
      "outputs": [],
      "source": [
        "!python tools/demo.py image -n yolox-s -c /content/YOLOX/weights/yolox_s.pth --path assets/dog.jpg --conf 0.25 --nms 0.45 --tsize 640 --save_result --device gpu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "agGfpd5ICUkI"
      },
      "outputs": [],
      "source": [
        "# Demo for Video\n",
        "!python tools/demo.py video -n yolox-s -c /content/YOLOX/weights/yolox_s.pth --path /content/YOLOX/video.mp4 --conf 0.60 --nms 0.45 --tsize 640 --save_result --device gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "SW1BoDmqXgoM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "6Ve3jYLNb4aX"
      },
      "outputs": [],
      "source": [
        "!cp /content/YOLOX/YOLOX_outputs/yolox_s/vis_res/2021_11_19_14_01_21/video.mp4 /content/drive/MyDrive/video.mp4\n",
        "#La commande cp est une commande Unix utilisée pour copier des fichiers ou des répertoires d'un emplacement à un autre."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXnB3MZyuY8p"
      },
      "source": [
        "#WebCAM Demo- Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPbil551uY8p"
      },
      "outputs": [],
      "source": [
        "# import dependencies\n",
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import cv2\n",
        "import numpy as np\n",
        "import PIL\n",
        "import io\n",
        "import html\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0Lk1LG7_Tkw"
      },
      "source": [
        "**js_to_image(js_reply) :**\n",
        "\n",
        "Convertit une image encodée en base64 (provenant de JavaScript) en une image OpenCV (BGR).\n",
        "Décodage de l'image en base64 en bytes.\n",
        "Transformation des bytes en tableau NumPy.\n",
        "Décodage du tableau NumPy en image OpenCV.\n",
        "\n",
        "**bbox_to_bytes(bbox_array) :**\n",
        "\n",
        "Convertit une image contenant un rectangle de détection (boîte englobante) en une chaîne base64.\n",
        "Convertit le tableau NumPy en image PIL.\n",
        "Sauvegarde l'image en format PNG dans un buffer mémoire.\n",
        "Encode l'image en base64 pour une utilisation dans un flux vidéo ou une interface web."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-C6pMlLOuY8p"
      },
      "outputs": [],
      "source": [
        "# function to convert the JavaScript object into an OpenCV image\n",
        "def js_to_image(js_reply):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          js_reply: JavaScript object containing image from webcam\n",
        "  Returns:\n",
        "          img: OpenCV BGR image\n",
        "  \"\"\"\n",
        "  # decode base64 image\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  # convert bytes to numpy array\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  # decode numpy array into OpenCV BGR image\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "  return img\n",
        "\n",
        "# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n",
        "def bbox_to_bytes(bbox_array):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n",
        "  Returns:\n",
        "        bytes: Base64 image byte string\n",
        "  \"\"\"\n",
        "  # convert array into PIL image\n",
        "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  # format bbox into png for return\n",
        "  bbox_PIL.save(iobuf, format='png')\n",
        "  # format return string\n",
        "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "  return bbox_bytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stbAKKXu_b4v"
      },
      "source": [
        "**1. video_stream() :**\n",
        "Crée un flux vidéo en utilisant la webcam de l'utilisateur avec un code JavaScript intégré.\n",
        "Capture les images de la vidéo et les convertit en format base64.\n",
        "Permet l'animation continue de la capture vidéo en utilisant window.requestAnimationFrame.\n",
        "Génère un élément vidéo HTML et le rend réactif à l'utilisateur (par exemple, en inversant l'image ou en permettant de stopper le flux).\n",
        "Ajoute un cadre (canvas) pour la capture d'images avec un contrôle pour arrêter la capture (via un clic).\n",
        "Retourne un flux vidéo en continu ou un cadre image à chaque appel.\n",
        "\n",
        "**2. video_frame(label, bbox) :**\n",
        "Appelle la fonction JavaScript stream_frame pour capturer un frame vidéo à partir du flux vidéo généré dans video_stream().\n",
        "Passe des informations comme un label et une boîte de détection (bbox).\n",
        "Retourne un objet contenant des informations sur le temps de création, affichage, capture de l'image, et l'image capturée elle-même en format base64."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDG53ARnuY8p"
      },
      "outputs": [],
      "source": [
        "# JavaScript to properly create our live video stream using our webcam as input\n",
        "def video_stream():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "\n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "\n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "\n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "\n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "\n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "\n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      // video.style.-webkit-transform = 'scaleX(-1)';\n",
        "      video.style.transform = 'scaleX(-1)';\n",
        "\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"user\"},\n",
        "          });\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "\n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML =\n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 640; //video.videoWidth;\n",
        "      captureCanvas.height = 480; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "\n",
        "      return stream;\n",
        "    }\n",
        "    async function stream_frame(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "\n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "\n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "\n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "\n",
        "      return {'create': preShow - preCreate,\n",
        "              'show': preCapture - preShow,\n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "\n",
        "def video_frame(label, bbox):\n",
        "  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "  return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIL5ePAwugo_"
      },
      "source": [
        "#WebCAM Demo- Predictor Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2wJbz12HUUS"
      },
      "outputs": [],
      "source": [
        "!pip install loguru\n",
        "#Loguru est une bibliothèque Python pour la gestion des logs.\n",
        "#Elle simplifie et rend plus flexible le processus de création et de gestion des logs dans vos applications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDKZXI9B8Anq"
      },
      "outputs": [],
      "source": [
        "#La bibliothèque thop est un outil Python conçu pour calculer la complexité de calcul d'un modèle de réseau de neurones. Elle fournit des méthodes pour évaluer le nombre d'opérations de type FLOP (Floating Point Operations)\n",
        "#d'un modèle donné. Cela permet de mesurer l'efficacité d'un modèle en termes de ressources de calcul nécessaires pour exécuter une inférence.\n",
        "!pip install thop # install the missing thop module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euVsn16JAPbN"
      },
      "source": [
        "Importation des bibliothèques : Utilisation de PyTorch, OpenCV, et des modules YOLOX pour la détection d'objets.\n",
        "\n",
        "# Classe Predictor :\n",
        "\n",
        "Initialisation : Charge un modèle YOLOX pré-entraîné et prépare l'environnement pour l'inférence.\n",
        "Méthode inference :\n",
        "Effectue l'inférence sur une image donnée.\n",
        "Redimensionne l'image et effectue des transformations avant de la passer au modèle.\n",
        "Applique un post-traitement pour obtenir les boîtes de détection et les scores de confiance.\n",
        "Optionnellement, dessine les résultats sur l'image.\n",
        "Retour : La méthode retourne les résultats sous forme de boîtes de détection, scores et identifiants de classes.\n",
        "\n",
        "Utilisation : Permet de charger un modèle, effectuer des prédictions sur des images ou vidéos, et afficher les résultats visuellement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gv0-tcZkuP_Z"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "from loguru import logger\n",
        "import sys\n",
        "\n",
        "from yolox.exp.build import get_exp_by_name\n",
        "from yolox.data.data_augment import ValTransform\n",
        "from yolox.data.datasets import COCO_CLASSES\n",
        "\n",
        "from yolox.utils import postprocess, vis\n",
        "\n",
        "def time_synchronized():\n",
        "    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
        "    return time.time()\n",
        "\n",
        "class_names = COCO_CLASSES\n",
        "\n",
        "IMAGE_EXT = [\".jpg\", \".jpeg\", \".webp\", \".bmp\", \".png\"]\n",
        "\n",
        "class Predictor():\n",
        "    def __init__(self, model='yolox-s', ckpt='yolox_s.pth', visual=True):\n",
        "        super(Predictor, self).__init__()\n",
        "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "        self.exp = get_exp_by_name(model)\n",
        "        self.test_size = self.exp.test_size\n",
        "        self.model = self.exp.get_model()\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "        checkpoint = torch.load(ckpt, map_location=\"cpu\")\n",
        "        self.model.load_state_dict(checkpoint[\"model\"])\n",
        "        self.preproc = ValTransform(legacy=False)\n",
        "\n",
        "\n",
        "    def inference(self, img, visual=True, conf=0.5, logger_=False):\n",
        "        img_info = {\"id\": 0}\n",
        "\n",
        "        if isinstance(img, str):\n",
        "            img_info[\"file_name\"] = os.path.basename(img)\n",
        "            img = cv2.imread(img)\n",
        "        else:\n",
        "            img_info[\"file_name\"] = None\n",
        "\n",
        "        img_info['img'] = img\n",
        "        height, width = img.shape[:2]\n",
        "        img_info[\"height\"], img_info[\"width\"], img_info[\"img\"]  = height, width, img\n",
        "        ratio = min(self.test_size[0] / img.shape[0], self.test_size[1] / img.shape[1])\n",
        "        img_info[\"ratio\"] = ratio\n",
        "        img, _ = self.preproc(img, None, self.test_size)\n",
        "        img = torch.from_numpy(img).unsqueeze(0)\n",
        "        img = img.float()\n",
        "\n",
        "        if self.device == torch.device('cuda'):\n",
        "            img = img.cuda()\n",
        "            # img = img.half()  # to FP16\n",
        "\n",
        "        with torch.no_grad():\n",
        "            t0 = time.time()\n",
        "            outputs = self.model(img)\n",
        "            outputs = postprocess(\n",
        "                outputs, self.exp.num_classes, self.exp.test_conf, self.exp.nmsthre\n",
        "                )[0].cpu().numpy()\n",
        "\n",
        "        img_info['boxes'] = outputs[:, 0:4]/ratio\n",
        "        img_info['scores'] = outputs[:, 4] * outputs[:, 5]\n",
        "        img_info['class_ids'] = outputs[:, 6]\n",
        "        img_info['box_nums'] = outputs.shape[0]\n",
        "\n",
        "        if visual:\n",
        "            img_info['visual'] = vis(img_info['img'], img_info['boxes'], img_info['scores'], img_info['class_ids'], conf, COCO_CLASSES)\n",
        "\n",
        "        if logger_:\n",
        "            logger.info(\"Infer time: {:.4f}s\".format(time.time() - t0))\n",
        "        return outputs, img_info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIt8TGrsPV2j"
      },
      "source": [
        "#WebCAM Demo- Live Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Klkitl_f_4Ov"
      },
      "source": [
        "**1. Initialisation du modèle :**\n",
        "ckpt : Charge le modèle pré-entraîné de YOLOX (version yolox_x.pth).\n",
        "predictor : Initialise un objet Predictor avec le modèle YOLOX et le checkpoint spécifié.\n",
        "\n",
        "\n",
        "**2. Démarrage du flux vidéo depuis la webcam :**\n",
        "video_stream() : Démarre la capture vidéo en utilisant la webcam de l'utilisateur via JavaScript.\n",
        "\n",
        "**3. Boucle de traitement vidéo :**\n",
        "label_html : Variable pour afficher l'état de la capture vidéo.\n",
        "bbox : Initialisation de la variable de la boîte de détection (vide au début).\n",
        "count : Un compteur, probablement pour la gestion du nombre de frames ou d'itérations.\n",
        "La boucle se déroule ainsi :\n",
        "\n",
        "video_frame(label_html, bbox) : Capture un frame vidéo à partir du flux et récupère le résultat sous forme d'image base64.\n",
        "js_to_image(js_reply[\"img\"]) : Convertit l'image base64 en une image OpenCV.\n",
        "cv2.flip(img, 1) : Inverse l'image horizontalement pour une vue miroir (utile pour les applications avec la webcam).\n",
        "Création d'un overlay pour la boîte de détection :\n",
        "Initialise une image vide bbox_array pour dessiner les boîtes de détection.\n",
        "Utilise predictor.inference(img) pour obtenir les informations de prédiction sur l'image.\n",
        "Dessin des boîtes sur l'image :\n",
        "La fonction vis() est utilisée pour superposer les boîtes de détection sur l'image. Elle prend en entrée les coordonnées des boîtes, les scores, les identifiants des classes, et un seuil de confiance (ici, 0.5).\n",
        "Traitement de la transparence :\n",
        "Met à jour la transparence dans bbox_array pour s'assurer que l'overlay est visible seulement là où il y a des boîtes.\n",
        "Conversion de l'overlay en bytes :\n",
        "La fonction bbox_to_bytes() est utilisée pour convertir l'overlay de la boîte de détection en une image PNG encodée en base64.\n",
        "Mise à jour de bbox :\n",
        "Le tableau de la boîte est mis à jour pour le prochain frame vidéo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "id": "lIM8cWZeDcoM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "fee844d0-0c7b-4d10-fc1d-7f7b6f80ac65"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Predictor' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-e680c0a53adc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/YOLOX/weights/yolox_x.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'yolox-x'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# start streaming video from webcam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Predictor' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "ckpt='/content/YOLOX/weights/yolox_x.pth'\n",
        "predictor = Predictor(model='yolox-x',ckpt=ckpt)\n",
        "\n",
        "\n",
        "# start streaming video from webcam\n",
        "video_stream()\n",
        "\n",
        "# label for video\n",
        "label_html = 'Capturing...'\n",
        "\n",
        "# initialze bounding box to empty\n",
        "bbox = ''\n",
        "count = 0\n",
        "while True:\n",
        "    js_reply = video_frame(label_html, bbox)\n",
        "    if not js_reply:\n",
        "        break\n",
        "\n",
        "    # convert JS response to OpenCV Image\n",
        "    img = js_to_image(js_reply[\"img\"])\n",
        "    img = cv2.flip(img, 1)\n",
        "\n",
        "    # create transparent overlay for bounding box\n",
        "    bbox_array = np.zeros([480,640,4], dtype=np.uint8)\n",
        "    _,img_info = predictor.inference(img)\n",
        "    # _,img_info = predictor.inference(img, visual=False)\n",
        "\n",
        "    bbox_array = vis(bbox_array, img_info['boxes'], img_info['scores'], img_info['class_ids'], 0.5, COCO_CLASSES)\n",
        "\n",
        "\n",
        "    bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n",
        "    # convert overlay of bbox into bytes\n",
        "    bbox_bytes = bbox_to_bytes(bbox_array)\n",
        "    # update bbox so next frame gets new overlay\n",
        "    bbox = bbox_bytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9AUp4LEwD0N"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "nTaUaYOQ_ov_",
        "CIL5ePAwugo_",
        "dIt8TGrsPV2j"
      ],
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}